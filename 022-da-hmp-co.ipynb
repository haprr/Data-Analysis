{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory|\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#importing the libraries required\nimport numpy as np \nimport pandas as pd \nimport numpy as np \nimport pandas as pd \nimport json\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom datetime import datetime\nimport glob\nimport seaborn as sns\nimport re\nimport os\nimport io\nfrom scipy.stats import boxcox\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#Import the data from CSV file, reading the csv into pandas dataframe\ndf = pd.read_csv('../input/us-accidents/US_Accidents_June20.csv')\n\n#To print the shape of data\nprint(\"The shape of data is:\",(df.shape))\n\n#displaying the first three columns of the dataframe\ndisplay(df.head(3))\n\ndf.columns\n\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drop 'Country' and 'Turning_Loop' as they have only one categorical class.\n\ncat_names = ['Country', 'Turning_Loop' ]\nprint(\"Unique count of categorical features:\")\nfor i in cat_names:\n  print(i,df[i].unique().size)\n\ndf = df.drop(['Country','Turning_Loop'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fix datetime type\ndf['Start_Time'] = pd.to_datetime(df['Start_Time'])\ndf['End_Time'] = pd.to_datetime(df['End_Time'])\ndf['Weather_Timestamp'] = pd.to_datetime(df['Weather_Timestamp'])\n\n# calculate duration as the difference between end time and start time in minute\ndf['Duration'] = df.End_Time - df.Start_Time \ndf['Duration'] = df['Duration'].apply(lambda x:round(x.total_seconds() / 60) )\nprint(\"The overall mean duration is: \", (round(df['Duration'].mean(),3)), 'min')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# average difference between weather time and start time\n\nprint(\"Mean difference between 'Start_Time' and 'Weather_Timestamp': \", \n(df.Weather_Timestamp - df.Start_Time).mean())\n\n#Since the 'Weather_Timestamp' is almost as same as 'Start_Time', we can just keep 'Start_Time'.\n\ndf = df.drop([\"Weather_Timestamp\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# mapping 'Start_Time' to 'Year', 'Month', 'Weekday', 'Day' (in a year), 'Hour', and 'Minute' (in a day).\n\n#extracting year from start time\ndf['Year'] = df['Start_Time'].dt.year\n\nnmonth = df['Start_Time'].dt.month\n\ndf['Month'] = nmonth\n\ndf['Weekday']= df['Start_Time'].dt.weekday\n\ndays_each_month = np.cumsum(np.array([0,31,28,31,30,31,30,31,31,30,31,30,31]))\n\nnday = [days_each_month[arg-1] for arg in nmonth.values]\n\nnday = nday + df[\"Start_Time\"].dt.day.values\n\ndf['Day'] = nday\n\ndf['Hour'] = df['Start_Time'].dt.hour\n\ndf['Minute']=df['Hour']*60.0+df[\"Start_Time\"].dt.minute\n\ndf.loc[:4,['Start_Time', 'Year', 'Month', 'Weekday', 'Day', 'Hour', 'Minute']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# show distinctive weather conditions \nweather ='!'.join(df['Weather_Condition'].dropna().unique().tolist())\nweather = np.unique(np.array(re.split(\n    \"!|\\s/\\s|\\sand\\s|\\swith\\s|Partly\\s|Mostly\\s|Blowing\\s|Freezing\\s\", weather))).tolist()\nprint(\"Weather Conditions: \", weather)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Clear'] = np.where(df['Weather_Condition'].str.contains('Clear', case=False, na = False), 1, 0)\ndf['Cloud'] = np.where(df['Weather_Condition'].str.contains('Cloud|Overcast', case=False, na = False), 1, 0)\ndf['Rain'] = np.where(df['Weather_Condition'].str.contains('Rain|storm', case=False, na = False), 1, 0)\ndf['Heavy_Rain'] = np.where(df['Weather_Condition'].str.contains('Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms', case=False, na = False), 1, 0)\ndf['Snow'] = np.where(df['Weather_Condition'].str.contains('Snow|Sleet|Ice', case=False, na = False), 1, 0)\ndf['Heavy_Snow'] = np.where(df['Weather_Condition'].str.contains('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls', case=False, na = False), 1, 0)\ndf['Fog'] = np.where(df['Weather_Condition'].str.contains('Fog', case=False, na = False), 1, 0)\n\n# Assign NA to created weather features where 'Weather_Condition' is null.\nweather = ['Clear','Cloud','Rain','Heavy_Rain','Snow','Heavy_Snow','Fog']\nfor i in weather:\n  df.loc[df['Weather_Condition'].isnull(),i] = df.loc[df['Weather_Condition'].isnull(),'Weather_Condition']\n\ndf.loc[:,['Weather_Condition'] + weather]\n\ndf = df.drop(['Weather_Condition'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# group data by 'Airport_Code' and 'Start_Month' then fill NAs with majority value\n\nfrom collections import Counter\nweather_cat = ['Wind_Direction'] + weather\nprint(\"Count of missing values that will be dropped: \")\nfor i in weather_cat:\n      df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\n      print(i + \" : \" + df[i].isnull().sum().astype(str))\n\n# drop na\n#There's quite a few! However, given the size of the dataset is large, we can drop these from the set and focus on the values that we have.\n\ndf = df.dropna(subset=weather_cat)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# group data by 'Airport_Code' and 'Start_Month' then fill NAs with median value\n\nWeather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\n\nprint(\"The number of remaining missing values: \")\n\nfor i in Weather_data:\n      df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n      print( i + \" : \" + df[i].isnull().sum().astype(str))\n    \n#There still are some missing values but much less. We did dropna for these features.\n\ndf = df.dropna(subset=Weather_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Weather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\n\nprint(\"The number of remaining missing values: \")\n\nfor i in Weather_data:\n      df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n      print( i + \" : \" + df[i].isnull().sum().astype(str))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Two differences are obvious in the above plots. The first is that the overall duration and impacted distance of accidents reported by Bing are much longer than those by MapQuest. Second, same severity level holds different meanings for MapQuest and Bing. MapQuest seems to have a clear and strict threshold for severity level 4, cases of which nevertheless only account for a tiny part of the whole dataset. Bing, on the other hand, doesn't seem to have a clear-cut threshold, especially regards duration, but the data is more balanced.\n\nIt is hard to choose one and we definitely can't use both. I decided to select MapQuest because serious accidents are we really care about and the sparse data of such accidents is the reality we have to confront.\n\nFinally, droping the data reported from Bing and 'Source' column."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[df['Source']==\"MapQuest\",]\ndf = df.drop(['Source'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#For categorical weather features, majority rather than median will be used to replace missing values.\n\n# group data by 'Airport_Code' and 'Start_Month' then filling NAs with majority value\n\n'''from collections import Counter\nweather_cat = ['Wind_Direction'] + weather\nprint(\"Count of missing values that will be dropped: \")\nfor i in weather_cat:\n  df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(Counter(x).most_common()[0][0]) if all(x.isnull())==False else x)\n  print(i + \" : \" + df[i].isnull().sum().astype(str))\n\ndf = df.dropna(subset=weather_cat)'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here, we count the number of NaN values present in our dataset\ndf.isnull().count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Features 'ID' doesn't provide any useful information about accidents themselves. 'TMC', 'Distance(mi)', 'End_Time' (we have start time), 'Duration', 'End_Lat', and 'End_Lng'(we have start location) can be collected only after the accident has already happened and hence cannot be predictors for serious accident prediction. For 'Description', the POI features have already been extracted from it by dataset creators. Let's get rid of these features first.\n\ndf = df.drop(['ID','TMC','Description','Distance(mi)', 'End_Time',\n              'End_Lat', 'End_Lng'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Precipitation_NA'] = 0\ndf.loc[df['Precipitation(in)'].isnull(),'Precipitation_NA'] = 1\ndf['Precipitation(in)'] = df['Precipitation(in)'].fillna(df['Precipitation(in)'].median())\ndf.loc[:5,['Precipitation(in)','Precipitation_NA']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The counts of missing values in some features are much smaller compared to the total sample. \n#It is convenient to drop rows with missing values in these columns.\n\ndf = df.dropna(subset=['City','Zipcode','Airport_Code',\n                       'Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# group data by 'Airport_Code' and 'Start_Month' then fill NAs with majority value\n\nWeather_data=['Temperature(F)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)']\nprint(\"The number of remaining missing values: \")\nfor i in Weather_data:\n      df[i] = df.groupby(['Airport_Code','Month'])[i].apply(lambda x: x.fillna(x.median()))\n      print( i + \" : \" + df[i].isnull().sum().astype(str))\n\n    \n#There still are some missing values but much less. Just dropna by these features for the sake of simplicity.\n\ndf = df.dropna(subset=Weather_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_year = df[(df['Start_Time'] > '2016-08-23') &\n                (df['Start_Time'] <= '2019-08-23')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Severity4'] = 0\ndf.loc[df['Severity'] == 4, 'Severity4'] = 1\ndf.Severity4.value_counts()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Severity'], axis = 1)\ndf_bl = pd.concat([df[df['Severity4']==1].sample(100000, replace = True),\n                   df[df['Severity4']==0].sample(100000)], axis=0)\nprint('resampled data:', df_bl.Severity4.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl.Year = df_bl.Year.astype(str)\nsns.countplot(x='Year', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Year (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a dataframe used to plot heatmap\ndf_date = df.loc[:,['Start_Time','Severity4']]         # create a new dateframe only containing time and severity\ndf_date['date'] = df_date['Start_Time'].dt.normalize() # keep only the date part of start time\ndf_date = df_date.drop(['Start_Time'], axis = 1)\ndf_date = df_date.groupby('date').sum()                # sum the number of accidents with severity level 4 by date\ndf_date = df_date.reset_index().drop_duplicates()\n\n# join the dataframe with full range of date from 2016 to 2019\nfull_date = pd.DataFrame(pd.date_range(start=\"2016-01-02\",end=\"2019-12-31\"))    \ndf_date = full_date.merge(df_date, how = 'left',left_on = 0, right_on = 'date')\ndf_date['date'] = df_date.iloc[:,0]\ndf_date = df_date.fillna(0)\ndf_date = df_date.iloc[:,1:].set_index('date')\n\n# group by date\ngroups = df_date['Severity4'].groupby(pd.Grouper(freq='A'))\nyears = pd.DataFrame()\nfor name, group in groups:\n  years[name.year] = group.values\n\n# plot\nyears = years.T\nplt.matshow(years, interpolation=None, aspect='auto')\nplt.title('Time Heatmap of Accident with Severity Level 4 (raw data)', y=1.2, fontsize=15)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.loc[(df['Year']==2019) & (df['Month']!=1) & (df['Month']!=2),:]\ndf = df.drop(['Year', 'Start_Time'], axis=1)\ndf['Severity4'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), df[df['Severity4']==0].sample(40000)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nsns.countplot(x='Month', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title('Count of Accidents by Month (resampled data)', size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"period_features = ['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\nfig, axs = plt.subplots(ncols=1, nrows=4, figsize=(13, 5))\n\nplt.subplots_adjust(wspace = 0.5)\nfor i, feature in enumerate(period_features, 1):    \n    plt.subplot(1, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in\\n{} Feature'.format(feature), size=13, y=1.05)\nfig.suptitle('Count of Accidents by Period-of-Day (resampled data)',y=1.08, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Minute_Freq'] = df.groupby(['Minute'])['Minute'].transform('count')\ndf['Minute_Freq'] = df['Minute_Freq']/df.shape[0]*24*60\ndf['Minute_Freq'] = df['Minute_Freq'].apply(lambda x: np.log(x+1))\n\n# resampling\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nsns.violinplot(x='Minute_Freq', y=\"Severity4\", data=df_bl, palette=\"Set2\")    \nplt.xlabel('Minute_Fre', size=12, labelpad=3)\nplt.ylabel('Severity4', size=12, labelpad=3)    \nplt.tick_params(axis='x', labelsize=12)\nplt.tick_params(axis='y', labelsize=12)\nplt.title('Minute Frequency by Severity (resampled data)', size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(6,5))\nchart = sns.countplot(x='Timezone', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Timezone (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', \n                      data=df_bl ,palette=\"Set2\", order=df_bl['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(25,5))\nchart = sns.countplot(x='State', hue='Severity4', data=df_bl ,palette=\"Set2\", order=df_bl[df_bl['Severity4']==1]['State'].value_counts().index)\nplt.title(\"Count of Accidents in State\\nordered by serious accidents' count (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q censusdata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import censusdata\n\n# download data\ncounty = censusdata.download('acs5', 2018, censusdata.censusgeo([('county', '*')]),\n                                   ['DP05_0001E',  'DP03_0019PE','DP03_0021PE','DP03_0022PE','DP03_0062E'],\n                                   tabletype='profile')\n# rename columns\ncounty.columns = ['Population_County','Drive_County','Transit_County','Walk_County','MedianHouseholdIncome_County']\ncounty = county.reset_index()\n# extract county name and convert them to lowercase\ncounty['County_y'] = county['index'].apply(lambda x : x.name.split(' County')[0].split(',')[0]).str.lower()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"county.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert all county name to lowercase \ndf['County'] = df['County'].str.lower()\n\n# left join df with census data\ndf = df.merge(county, left_on = 'County', right_on='County_y',how = 'left').drop('County_y', axis = 1)\njoin_var = county.columns.to_list()[:-1]\n\n# check how many miss match we got\nprint('Count of missing values before: ', df[join_var].isnull().sum())\n\n# add \"city\" and match again\ndf_city = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_city['County_city'] = df_city['County'].apply(lambda x : x + ' city')\ndf_city = df_city.merge(county,left_on= 'County_city',right_on = 'County_y', how = 'left').drop(['County_city','County_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_city), axis=0)\n\n# add \"parish\" and match again\ndf_parish = df[df['Walk_County'].isnull()].drop(join_var, axis=1)\ndf_parish['County_parish'] = df_parish['County'].apply(lambda x : x + ' parish')\ndf_parish = df_parish.merge(county,left_on= 'County_parish',right_on = 'County_y', how = 'left').drop(['County_parish','County_y'], axis=1)\ndf = pd.concat((df[df['Walk_County'].isnull()==False], df_parish), axis=0)\nprint('Count of missing values after: ', df[join_var].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop na\ndf = df.drop('index', axis = 1).dropna()\n\n# log-transform\nfor i in ['Population_County','Transit_County','Walk_County']:\n    df[i + '_log'] = df[i].apply(lambda x: np.log(x+1))\ndf = df.drop(['Population_County','Transit_County','Walk_County'], axis = 1)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n# plot\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\ncensus_features = ['Population_County_log','Drive_County','Transit_County_log','Walk_County_log','MedianHouseholdIncome_County']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(census_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nfig.suptitle('Density of Accidents in Census Data (resampled data)', fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a list of top 40 most common words in street name\nst_type =' '.join(df['Street'].unique().tolist()) # flat the array of street name\nst_type = re.split(\" |-\", st_type) # split the long string by space and hyphen\nst_type = [x[0] for x in Counter(st_type).most_common(40)] # select the 40 most common words\nprint('the 40 most common words')\nprint(*st_type, sep = \", \") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove some irrelevant words and add spaces and hyphen back\nst_type= [' Rd', ' St', ' Dr', ' Ave', ' Blvd', ' Ln', ' Highway', ' Pkwy', ' Hwy', \n          ' Way', ' Ct', 'Pl', ' Road', 'US-', 'Creek', ' Cir', 'Hill', 'Route', \n          'I-', 'Trl', 'Valley', 'Ridge', 'Pike', ' Fwy', 'River']\nprint(*st_type, sep = \", \")  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for each word create a boolean column\nfor i in st_type:\n  df[i.strip()] = np.where(df['Street'].str.contains(i, case=True, na = False), 1, 0)\ndf.loc[df['Road']==1,'Rd'] = 1\ndf.loc[df['Highway']==1,'Hwy'] = 1\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nstreet_corr  = df_bl.loc[:,['Severity4']+[x.strip() for x in st_type]].corr()\nplt.figure(figsize=(20,15))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(street_corr, annot=True, cmap=cmap, center=0).set_title(\"Correlation (resampled data)\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_list = street_corr.index[street_corr['Severity4'].abs()<0.1].to_list()\ndf = df.drop(drop_list, axis=1)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(5,5))\nchart = sns.countplot(x='Side', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents by Side (resampled data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Start_Lat', 'Start_Lng']\nfig, axs = plt.subplots(ncols=1, nrows=2, figsize=(10, 5))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(1, 2, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Distribution of Accidents by Latitude and Longitude\\n(resampled data)', fontsize=18,y=1.08)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_4 = df[df['Severity4']==1]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fre_list = ['Street', 'City', 'County', 'Zipcode', 'Airport_Code']\nfor i in fre_list:\n  newname = i + '_Freq'\n  df[newname] = df.groupby([i])[i].transform('count')\n  df[newname] = df[newname]/df.shape[0]*df[i].unique().size\n  df[newname] = df[newname].apply(lambda x: np.log(x+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(10, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfig.suptitle('Location Frequency by Severity (resampled data)', fontsize=16)\nfor i, feature in enumerate(fre_list, 1): \n    feature = feature + '_Freq'   \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity4', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{}'.format(feature), size=16, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(fre_list, axis  = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pressure_bc']= boxcox(df['Pressure(in)'].apply(lambda x: x+1),lmbda=6)\ndf['Visibility_bc']= boxcox(df['Visibility(mi)'].apply(lambda x: x+1),lmbda = 0.1)\ndf['Wind_Speed_bc']= boxcox(df['Wind_Speed(mph)'].apply(lambda x: x+1),lmbda=-0.2)\ndf = df.drop(['Pressure(in)','Visibility(mi)','Wind_Speed(mph)'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\ndf_bl['Severity4'] = df_bl['Severity4'].astype('category')\nnum_features = ['Temperature(F)', 'Humidity(%)', 'Pressure_bc', 'Visibility_bc', 'Wind_Speed_bc']\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.2)\nfor i, feature in enumerate(num_features, 1):    \n    plt.subplot(2, 3, i)\n    sns.violinplot(x=feature, y=\"Severity4\", data=df_bl, palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Severity', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n\n    plt.title('{} Feature by Severity'.format(feature), size=14, y=1.05)\nfig.suptitle('Density of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(15, 10))\nplt.subplots_adjust(hspace=0.4,wspace = 0.6)\nfor i, feature in enumerate(weather, 1):    \n    plt.subplot(2, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in \\n {} Feature'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents by Weather Features (resampled data)', fontsize=18)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,5))\nchart = sns.countplot(x='Wind_Direction', hue='Severity4', data=df_bl ,palette=\"Set2\")\nplt.title(\"Count of Accidents in Wind Direction (resample data)\", size=15, y=1.05)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POI_features = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal']\n\nfig, axs = plt.subplots(ncols=3, nrows=4, figsize=(15, 10))\n\nplt.subplots_adjust(hspace=0.5,wspace = 0.5)\nfor i, feature in enumerate(POI_features, 1):    \n    plt.subplot(3, 4, i)\n    sns.countplot(x=feature, hue='Severity4', data=df_bl ,palette=\"Set2\")\n    \n    plt.xlabel('{}'.format(feature), size=12, labelpad=3)\n    plt.ylabel('Accident Count', size=12, labelpad=3)    \n    plt.tick_params(axis='x', labelsize=12)\n    plt.tick_params(axis='y', labelsize=12)\n    \n    plt.legend(['0', '1'], loc='upper right', prop={'size': 10})\n    plt.title('Count of Severity in {}'.format(feature), size=14, y=1.05)\nfig.suptitle('Count of Accidents in POI Features (resampled data)',y=1.02, fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df= df.drop(['Bump','Give_Way','No_Exit','Roundabout','Traffic_Calming'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# one-hot encoding\ndf[period_features] = df[period_features].astype('category')\ndf = pd.get_dummies(df, columns=period_features, drop_first=True)\n\n# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n\n# plot correlation\ndf_bl['Severity4'] = df_bl['Severity4'].astype(int)\nplt.figure(figsize=(20,20))\ncmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\nsns.heatmap(df_bl.corr(), annot=True,cmap=cmap, center=0).set_title(\"Correlation Heatmap\", fontsize=16)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.drop(['Day','Minute','Population_County_log','City_Freq','Civil_Twilight_Night',\n              'Nautical_Twilight_Night'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.replace([True, False], [1,0])\n\ncat = ['Side','State','Timezone','Wind_Direction', 'Weekday', 'Month', 'Hour']\ndf[cat] = df[cat].astype('category')\ndf = pd.get_dummies(df, columns=cat, drop_first=True)\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resample again\ndf_bl = pd.concat([df[df['Severity4']==1].sample(40000, replace = True), \n                   df[df['Severity4']==0].sample(40000)], axis=0)\n# split X, y\nX = df_bl.drop('Severity4', axis=1)\ny= df_bl['Severity4']\n\n# Standardizing the features based on unit variance\nfrom sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)\n\n# split train test\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\\\n  X, y, test_size=0.20, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression with default setting.\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(max_iter=10000,random_state=42)\nclf.fit(X_train, y_train)\n\naccuracy_train = clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import multilabel_confusion_matrix\n\ny_pred = clf.predict(X_test)\n\nconfmat = multilabel_confusion_matrix(y_true=y_test, y_pred=y_pred,\n                           labels=[1])\n\nconf_matrix = pd.DataFrame(data=confmat[0],\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n-Default Logistic Regression\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Grid Search\nfrom sklearn.model_selection import GridSearchCV\nLR_grid = {\n           'penalty':            ['none','l2'],\n           'C':                  [0.001,.009,0.01,.09,1,5,10,25],\n           'max_iter': [1000, 10000, 100000]\n           }\nCV_LR = GridSearchCV(estimator=LogisticRegression(random_state=42), param_grid = LR_grid,scoring = 'accuracy',cv=5)\nCV_LR.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters: ', CV_LR.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nCV_LR_clf = LogisticRegression(C=0.09, max_iter=1000, penalty='l2')\nCV_LR_clf.fit(X_train, y_train)\naccuracy_train = CV_LR_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\naccuracy_test = CV_LR.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nDT_grid = { 'min_samples_split': [5,10, 20, 30, 40], \n          'max_features': [None, 'log2', 'sqrt']}\nCV_DT = GridSearchCV(DecisionTreeClassifier(random_state=42), DT_grid, verbose=1, cv=3)\nCV_DT.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn import tree\n# Training step, on X_train with y_train\ntree_clf = tree.DecisionTreeClassifier(min_samples_split = 5)\ntree_clf = tree_clf.fit(X_train,y_train)\n\ntree_accuracy_train = tree_clf.score(X_train, y_train)\nprint(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\ntree_accuracy_test = tree_clf.score(X_test,y_test)\nprint(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = tree_clf.predict(X_test)\n\nconfmat = confusion_matrix(y_true=y_test, y_pred=prediction)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n Decision Tree\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], \n                           index=df.drop('Severity4',axis=1).columns)\n\nimportances.iloc[:,0] = tree_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n\nplt.figure(figsize=(10, 10))\nsns.barplot(x='importance', y=importances30.index, data=importances30)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Decision Tree Classifier Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20, 10))\ntree.plot_tree(tree_clf, max_depth=4, fontsize=10,\n               feature_names=df.drop('Severity4',axis =1).columns.to_list(),\n               class_names = True, filled=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = { \n    'n_estimators'     : [30,40,50],\n    'max_depth'        : [20,30,40]\n}\nCV_clf = GridSearchCV(estimator=RandomForestRegressor(), param_grid=param_grid,cv=4)\nCV_clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best Parameters: ', CV_clf.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import accuracy_score\nrf_clf = RandomForestRegressor(max_depth=40,n_estimators=50)\nrf_clf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = lambda x: 1 if x>=0.5 else 0\ntrain_pred = np.array(list(map(f, rf_clf.predict(X_train))))\ntest_pred = np.array(list(map(f, rf_clf.predict(X_test))))\n\nrf_train_accuracy = accuracy_score(y_train, train_pred)\nprint(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\nrf_test_accuracy = accuracy_score(y_test, test_pred)\nprint(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"confmat = confusion_matrix(y_true=y_test, y_pred=test_pred)\n\nconf_matrix = pd.DataFrame(data=confmat,\n                           columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\").set_title(\n    \"Confusion Matrix (resampled data)\\n Random Forest\", fontsize=16)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=['importance'], index=df.drop('Severity4',axis=1).columns)\n\nimportances.iloc[:,0] = rf_clf.feature_importances_\n\nimportances.sort_values(by='importance', inplace=True, ascending=False)\nimportances30 = importances.head(30)\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x='importance', y=importances30.index, data=importances30)\n\nplt.xlabel('')\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=10)\nplt.title('Random Forest Classifier Feature Importance', size=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}